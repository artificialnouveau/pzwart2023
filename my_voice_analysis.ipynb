{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae11a2-a5f2-4aad-bf43-88fbc24fee8f",
   "metadata": {},
   "source": [
    "### Import libraries and create custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42ba2e-625f-4030-8a87-3212e33f5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display, HTML\n",
    "from pydub import AudioSegment\n",
    "from pytube import YouTube\n",
    "from moviepy.editor import *\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from simple_diarizer.diarizer import Diarizer\n",
    "from simple_diarizer.utils import (check_wav_16khz_mono, convert_wavfile,\n",
    "                                   waveplot, combined_waveplot, waveplot_perspeaker)\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "import soundfile as sf\n",
    "from tqdm.autonotebook import tqdm\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def split_into_sentences(segments):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for segment in segments:\n",
    "        word, start, end = segment['text'], segment['start'], segment['end']\n",
    "        current_sentence.append((word, start, end))\n",
    "        if word.endswith('.'):\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    return sentences\n",
    "\n",
    "def segment_audio(filename, sentences):\n",
    "    audio = AudioSegment.from_file(filename)\n",
    "    segments = []\n",
    "    for sentence in sentences:\n",
    "        start = int(sentence[0][1] * 1000)  # Convert to milliseconds\n",
    "        end = int(sentence[-1][2] * 1000)\n",
    "        segment = audio[start:end]\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def save_audio_segment(segment, filename):\n",
    "    segment.export(filename, format=\"wav\")\n",
    "\n",
    "def analyze_audio(filename, analyze_by_sentence=False):\n",
    "    mysp = __import__(\"my-voice-analysis\")\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(filename)\n",
    "    text = result[\"text\"]\n",
    "    print('Transcribed text: ', text)\n",
    "    print('')\n",
    "\n",
    "    file = os.path.basename(filename).replace('.wav', '')\n",
    "    directory = os.path.dirname(filename)\n",
    "\n",
    "    if analyze_by_sentence:\n",
    "        sentences = split_into_sentences(result[\"segments\"])\n",
    "        audio_segments = segment_audio(filename, sentences)\n",
    "\n",
    "        analyses = []\n",
    "\n",
    "        for i, segment in enumerate(audio_segments):\n",
    "            temp_filename = os.path.join(directory, f\"temp_segment_{i}.wav\")\n",
    "            save_audio_segment(segment, temp_filename)\n",
    "\n",
    "            try:\n",
    "                temp_filename_ = os.path.basename(temp_filename).replace('.wav', '')\n",
    "                gender, emotion = mysp.myspgend(temp_filename_, directory)\n",
    "                dataset, *analysis_results = mysp.mysptotal(temp_filename_, directory)\n",
    "            except:\n",
    "                gender = emotion = \"Unknown\"\n",
    "                analysis_results = [\"Unknown\"] * 13\n",
    "\n",
    "            analyses.append({\n",
    "                \"sentence\": ' '.join([word[0] for word in sentences[i]]),\n",
    "                \"gender\": gender,\n",
    "                \"emotion\": emotion,\n",
    "                **dict(zip([\"number_of_syllables\", \"number_of_pauses\", \"rate_of_speech\", \"articulation_rate\", \"speaking_duration\", \"original_duration\", \"balance\", \"f0_mean\", \"f0_std\", \"f0_median\", \"f0_min\", \"f0_max\", \"f0_quantile25\", \"f0_quan75\"], analysis_results))\n",
    "            })\n",
    "\n",
    "            os.remove(temp_filename)\n",
    "    else:\n",
    "        try:\n",
    "            gender, emotion = mysp.myspgend(file, directory)\n",
    "            dataset, *analysis_results = mysp.mysptotal(file, directory)\n",
    "        except:\n",
    "            gender = emotion = \"Unknown\"\n",
    "            analysis_results = [\"Unknown\"] * 13\n",
    "\n",
    "        analyses = [{\n",
    "            \"filename\": filename,\n",
    "            \"text\": text,\n",
    "            \"gender\": gender,\n",
    "            \"emotion\": emotion,\n",
    "            **dict(zip([\"number_of_syllables\", \"number_of_pauses\", \"rate_of_speech\", \"articulation_rate\", \"speaking_duration\", \"original_duration\", \"balance\", \"f0_mean\", \"f0_std\", \"f0_median\", \"f0_min\", \"f0_max\", \"f0_quantile25\", \"f0_quan75\"], analysis_results))\n",
    "        }]\n",
    "\n",
    "    return analyses\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Remove spaces and special characters from the filename.\"\"\"\n",
    "    filename = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '', filename.replace(\" \", \"_\"))\n",
    "    return filename\n",
    "\n",
    "def download_youtube_audio(url, output_path=\"downloaded_audio\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Download YouTube video\n",
    "    yt = YouTube(url)\n",
    "    video_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_file = video_stream.download(output_path=output_path)\n",
    "\n",
    "    # Sanitize filename\n",
    "    sanitized_filename = sanitize_filename(yt.title) + '.wav'\n",
    "    wav_filepath = os.path.join(output_path, sanitized_filename)\n",
    "\n",
    "    audio_clip = AudioFileClip(audio_file)\n",
    "    audio_clip.write_audiofile(wav_filepath, fps=44100, nbytes=2, codec='pcm_s16le')\n",
    "\n",
    "    # Remove the original download (if it's not a wav file)\n",
    "    if not audio_file.endswith('.wav'):\n",
    "        os.remove(audio_file)\n",
    "\n",
    "    print(f\"Audio downloaded and converted to WAV: {wav_filepath}\")\n",
    "    return wav_filepath\n",
    "\n",
    "\n",
    "def shorten_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) > 20:\n",
    "        return ' '.join(words[:5]) + ' ... ' + ' '.join(words[-5:])\n",
    "    return sentence\n",
    "\n",
    "    \n",
    "def freq_plot(analysis_results):\n",
    "    data = analysis_results\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert columns to numeric as needed\n",
    "    numeric_cols = ['f0_mean', 'f0_std', 'f0_median', 'f0_min', 'f0_max', 'f0_quantile25', 'f0_quan75']\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Use color to represent emotion - can map each emotion to a color\n",
    "    # Ensure emotions have random colors\n",
    "    unique_emotions = df['emotion'].unique()\n",
    "    emotion_colors = {emotion: \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for emotion in unique_emotions}\n",
    "    \n",
    "    # Plotting only f0_median with quantiles as confidence\n",
    "    plt.figure(figsize=(15, max(6, len(df) / 2)))  # Adjust figure size as needed\n",
    "    \n",
    "    # Plotting each point and its confidence interval\n",
    "    for i, row in df.iterrows():\n",
    "        plt.plot(row['f0_median'], i, 'o', color=emotion_colors[row['emotion']])\n",
    "        plt.hlines(i, row['f0_quantile25'], row['f0_quan75'], color=emotion_colors[row['emotion']], alpha=0.3)\n",
    "\n",
    "    # Shorten sentences if they are too long\n",
    "    df['sentence_short'] = df['sentence'].copy()\n",
    "    df['sentence_short'] = df['sentence_short'].apply(shorten_sentence)\n",
    "    \n",
    "    plt.yticks(ticks=df.index, labels=df['sentence_short'])\n",
    "    plt.title(\"Median Fundamental Frequency (F0) with Confidence Intervals for Each Sentence\")\n",
    "    plt.ylabel(\"Sentences\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    \n",
    "    # Create a legend for the emotions\n",
    "    legend_elements = [Line2D([0], [0], color=color, lw=4, label=emotion) for emotion, color in emotion_colors.items()]\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def heat_plot(analysis_results):\n",
    "    data = analysis_results\n",
    "    df = pd.DataFrame(data)\n",
    "    # Convert columns to numeric as needed\n",
    "    numeric_cols = ['number_of_syllables', 'number_of_pauses', 'rate_of_speech', \n",
    "                    'articulation_rate', 'speaking_duration', 'f0_mean', 'f0_std', \n",
    "                    'f0_median', 'f0_min', 'f0_max', 'f0_quantile25', 'f0_quan75']\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df['sentence_short'] = df['sentence'].copy()\n",
    "    df['sentence_short'] = df['sentence_short'].apply(shorten_sentence)\n",
    "\n",
    "    # Ensure all columns are numeric for the heatmap\n",
    "    heatmap_data = df[numeric_cols]\n",
    "    \n",
    "    # Drop any rows with NaN values (if necessary)\n",
    "    heatmap_data = heatmap_data.dropna()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(len(heatmap_data.columns), 15))  # Adjust figure size as needed\n",
    "    sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='g', yticklabels=df['sentence_short'])  # 'g' format to avoid scientific notation\n",
    "    plt.title(\"Heatmap of Fundamental Frequencies and Speech Features\")\n",
    "    plt.ylabel(\"Sentences\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def trim_wav_file(input_file_path, start_time_s, end_time_s=None):\n",
    "    \"\"\"\n",
    "    Trims a .wav file from start_time_s and optionally to end_time_s, then saves it as a new file.\n",
    "\n",
    "    Args:\n",
    "    input_file_path (str): Path to the input .wav file.\n",
    "    start_time_s (int): Start time in seconds to begin trimming.\n",
    "    end_time_s (int, optional): End time in seconds to stop trimming. If None, the audio is trimmed only from the start.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_wav(input_file_path)\n",
    "\n",
    "    # Determine the end time for trimming\n",
    "    if end_time_s is not None:\n",
    "        trimmed_audio = audio[start_time_s * 1000:end_time_s * 1000]\n",
    "        end_time_label = f\"_to_{end_time_s}s\"\n",
    "    else:\n",
    "        trimmed_audio = audio[start_time_s * 1000:]\n",
    "        end_time_label = \"\"\n",
    "    # Create the new file name with timestamps\n",
    "    file_name, file_extension = os.path.splitext(input_file_path)\n",
    "    new_file_name = f\"{file_name}_{start_time_s}s{end_time_label}{file_extension}\"\n",
    "\n",
    "    # Export the trimmed audio\n",
    "    trimmed_audio.export(new_file_name, format=\"wav\")\n",
    "\n",
    "    print(f\"Trimmed audio saved to {new_file_name}\")\n",
    "\n",
    "def joy_division(audio_file, interval = 10, amplify = 1.5):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Process the audio data\n",
    "    data = np.abs(y)\n",
    "    \n",
    "    # Calculate the number of 10-second intervals\n",
    "    duration_in_sec = len(y) / sr\n",
    "    num_intervals = int(np.ceil(duration_in_sec / 10))\n",
    "    \n",
    "    # Determine the number of samples per 10-second interval\n",
    "    samples_per_interval = 10 * sr\n",
    "    \n",
    "    # Determine the number of columns (adjust as needed for visualization)\n",
    "    num_columns = 75  # This is adjustable depending on the desired resolution\n",
    "    \n",
    "    # Reshape the data\n",
    "    target_size = num_intervals * samples_per_interval  # Total required size\n",
    "    data_padded = np.pad(data, (0, max(0, target_size - len(data))), 'constant')  # Pad data if needed\n",
    "    data_reshaped = data_padded[:target_size].reshape((num_intervals, -1))\n",
    "    \n",
    "    # Truncate or pad columns to fit the visualization\n",
    "    data_reshaped = data_reshaped[:, :min(data_reshaped.shape[1], num_columns)]\n",
    "    \n",
    "    # Create new Figure with black background\n",
    "    fig = plt.figure(figsize=(8, 8), facecolor='black')\n",
    "    \n",
    "    # Add a subplot with no frame\n",
    "    ax = plt.subplot(111, frameon=False)\n",
    "    \n",
    "    # Generate line plots\n",
    "    lines = []\n",
    "    X = np.linspace(-1, 1, num_columns)\n",
    "    G = 1.5 * np.exp(-4 * X * X)\n",
    "    for i in range(len(data_reshaped)):\n",
    "        xscale = 1 - i / 200.\n",
    "        lw = 1.5 - i / 100.0\n",
    "        line, = ax.plot(xscale * X, i + G * data_reshaped[i]*amplify, color=\"w\", lw=lw)\n",
    "        lines.append(line)\n",
    "    \n",
    "    # Set y limit\n",
    "    ax.set_ylim(-1, num_intervals)\n",
    "    \n",
    "    # No ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Titles\n",
    "    ax.text(0.5, 1.0, \" \", transform=ax.transAxes, ha=\"right\", va=\"bottom\", color=\"w\", family=\"sans-serif\", fontweight=\"light\", fontsize=16)\n",
    "    \n",
    "    def update(frame):\n",
    "        # Shift all data to the right\n",
    "        data_reshaped[:, 1:] = data_reshaped[:, :-1]\n",
    "    \n",
    "        # Update data for each line\n",
    "        for i in range(len(data_reshaped)):\n",
    "            lines[i].set_ydata(i + G * data_reshaped[i])\n",
    "    \n",
    "        return lines\n",
    "    \n",
    "    # Set up the animation\n",
    "    anim = animation.FuncAnimation(fig, update, frames=len(data_reshaped[0]), interval=interval)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_spectrogram(file_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path)\n",
    "    \n",
    "    # Compute the spectrogram\n",
    "    S = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
    "    \n",
    "    # Plot the spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(S, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.show()\n",
    "\n",
    "def plot_voice_pitch_with_rolling_average(audio_file, low_freq=75, high_freq=300, window_size=50):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "\n",
    "    # Apply a band-pass filter\n",
    "    sos = scipy.signal.butter(10, [low_freq, high_freq], 'bandpass', fs=sr, output='sos')\n",
    "    filtered_y = scipy.signal.sosfilt(sos, y)\n",
    "\n",
    "    # Extract the pitch (fundamental frequency)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=filtered_y, sr=sr)\n",
    "    pitch = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        pitch.append(pitches[index, t])\n",
    "\n",
    "    pitch = np.array(pitch)\n",
    "    pitch[pitch == 0] = np.nan  # Remove zeros\n",
    "\n",
    "    # Calculate the rolling average\n",
    "    rolling_avg = np.convolve(pitch, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    # Adjust the time axis for pitch and rolling average\n",
    "    time_pitch = np.linspace(0, len(y) / sr, len(pitch))\n",
    "    time_rolling_avg = np.linspace(window_size/(2*sr), len(y) / sr - window_size/(2*sr), len(rolling_avg))\n",
    "\n",
    "    # Plot the pitch and the rolling average over time\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(time_pitch, pitch, label='Pitch')\n",
    "    plt.plot(time_rolling_avg, rolling_avg, label='Rolling Average', color='red')\n",
    "    plt.title(\"Pitch and Rolling Average of the Voice over Time\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Pitch (Hz)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_mfcc(audio_file):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "\n",
    "    # Calculate MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Displaying the MFCCs\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    img = librosa.display.specshow(mfccs, x_axis='time', sr=sr)\n",
    "\n",
    "    # Add a color bar with label\n",
    "    plt.colorbar(img, format='%+2.0f dB')\n",
    "\n",
    "    # Set the title and labels\n",
    "    plt.title('MFCC')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "    plt.xlabel('Time (s)')\n",
    "\n",
    "    # Set y-axis ticks to correspond to MFCC coefficients\n",
    "    plt.yticks(np.arange(mfccs.shape[0]), [f'MFCC {i+1}' for i in range(mfccs.shape[0])])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, folder):\n",
    "    # Extract the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "\n",
    "    # Create the full file path\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Make a request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content to the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd479c-c07d-4b2b-8af2-426d3057a0ea",
   "metadata": {},
   "source": [
    "## Prepare your audio\n",
    "\n",
    "You can use your own audio. However, your audio files must be in *.wav format, recorded at 44 kHz sample frame and 16 bits of resolution.\n",
    "\n",
    "You also have the option to download the audio from youtube. This can either be of someone speaking or someone singing. \n",
    "\n",
    "Please select audio that is less than 10 mins long. \n",
    "\n",
    "After installing My-Voice-Analysis, copy the file myspsolution.praat from https://github.com/Shahabks/my-voice-analysis  \n",
    "\n",
    "and save in the directory where you will save audio files for analysis (or use the download_file function as seen below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ca00e-1c3e-41af-82ef-2d6b299e889d",
   "metadata": {},
   "source": [
    "### Download youtube audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4530327-0f69-4d6b-b991-7a142e1ae021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace or add your YouTube URLs\n",
    "youtube_urls = [\"https://youtu.be/InR69mIZMzA\", \"https://www.youtube.com/watch?v=uAPUkgeiFVY\", \"https://www.youtube.com/watch?v=Ez-L0qW9iGQ\"]\n",
    "for url in youtube_urls:\n",
    "    download_youtube_audio(url)\n",
    "\n",
    "download_file(url, 'downloaded_audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfbecc-2ffb-405a-a11e-76df5d1fbe3a",
   "metadata": {},
   "source": [
    "### Separate vocals from instruments\n",
    "\n",
    "If you are using audio from a song then you can separate the vocals from the instruments using spleeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb4f94-1c1f-4057-a41d-bf8bde0131d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = './downloaded_audio/'\n",
    "input_audio = './downloaded_audio/The_Heart_Part_5.wav'\n",
    "!spleeter separate -o {output_folder} {input_audio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c265189-52fa-4300-a160-410882ea16f8",
   "metadata": {},
   "source": [
    "### You can trim the wav file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f731b41-8b3c-4c10-8102-3c8f885cc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim_wav_file(file_path, start_time_seconds, end_time_seconds) <-- end_time_seconds is optional\n",
    "trim_wav_file('./downloaded_audio/The_Heart_Part_5.wav', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30548777-a5dc-41de-8a0b-4aaf5983f088",
   "metadata": {},
   "source": [
    "### Playback audio\n",
    "If the audio file is too long, then this audio plug-in will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647ced3-9741-468f-842f-b804779ed23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using backward slash, then add r in front of the filepath. If you are using forward slash, then no r necessary\n",
    "file_path = r\".\\downloaded_audio\\Prime_Minister_Trudeaus_message_on_Remembrance_Day.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45210-aec3-4ef5-8862-cd44d3aeec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(filename=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3188a78-3def-4e26-9ed7-70f2e5da5301",
   "metadata": {},
   "source": [
    "### Visualize Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d8c7c-6e2d-4c5c-9a09-4433775f6dbd",
   "metadata": {},
   "source": [
    "This script will display the spectrogram of the audio file, where the x-axis represents time, the y-axis represents frequency on a logarithmic scale, and the color intensity represents the amplitude of the frequency at each time point.\n",
    "\n",
    "This joy_division function visualizes an audio file in the style of the iconic album cover for Joy Division's \"Unknown Pleasures.\" It processes the audio data to create a series of waveforms, each representing a segment of the audio track. The visualization is interesting because it transforms the audio experience into a dynamic visual display, allowing the viewer to observe the variations and patterns in the audio data over time. This form of visualization can provide a novel perspective on the structure and rhythm of the audio, making it a compelling tool for both artistic expression and analytical exploration of sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5896f5-4c0d-47a6-bc55-41dad62954b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_division(file_path, interval = 10, amplify = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c6d80-adfe-4699-89ce-7b06b6fc4b8d",
   "metadata": {},
   "source": [
    "#### Spectrogram\n",
    "\n",
    "A spectrogram is a visual representation of the spectrum of frequencies of a sound signal as they vary with time. It shows how the energy of a signal is distributed across different frequencies over time. Here are the key elements that a spectrogram displays:\n",
    "\n",
    "1. **Frequency**: The vertical axis represents frequency, usually with lower frequencies at the bottom and higher frequencies at the top. This shows which frequencies are present in the audio signal at any given time.\n",
    "\n",
    "2. **Time**: The horizontal axis represents time, moving from left to right. This allows you to see how the frequency content of the sound changes over the duration of the recording.\n",
    "\n",
    "3. **Amplitude or Intensity**: The intensity of different frequencies at different times is represented by the color or brightness of each point in the spectrogram. Typically, darker or more vibrant colors indicate higher energy or amplitude at a particular frequency and time.\n",
    "\n",
    "A spectrogram is a powerful tool for analyzing the acoustic properties of sounds, particularly for understanding speech, music, and natural sounds. It is widely used in various fields, including linguistics (for phonetic analysis), musicology, bioacoustics, and sound engineering. By providing a visual snapshot of the sound, a spectrogram can reveal intricate details such as the harmonic structure of a musical note, the formant frequencies of vowels in speech, or the varying calls of birds and other animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69d261-0c85-46a4-9848-50caa2c3a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7428643-4dee-45fb-8451-9552e8b7caf0",
   "metadata": {},
   "source": [
    "#### MFCC \n",
    "\n",
    "Mel Frequency Cepstral Coefficients (MFCCs) are a feature representation widely used in the fields of audio and speech processing, especially in speech and speaker recognition systems. Here's what MFCCs show:\n",
    "\n",
    "    Frequency Features: MFCCs capture the power spectrum of a sound. They are derived from the cepstrum, which is the result of taking the log of the short-term power spectrum and then performing a Fourier transform.\n",
    "\n",
    "    Mel Scale Emphasis: The \"Mel\" in MFCC stands for the Mel scale, a scale that mimics the human ear's response more closely than the linearly-spaced frequency bands. This scale is important because human hearing does not perceive frequencies linearly; instead, it is more sensitive to changes in lower frequencies than higher ones. MFCCs transform the frequencies into the Mel scale, making them more representative of how humans hear sounds.\n",
    "\n",
    "    Representation of Timbral and Textural Characteristics: MFCCs are particularly good at representing the timbre or texture of audio signals, capturing aspects like the tone quality and the sound's color. This makes them very useful in distinguishing different sounds and understanding their characteristics.\n",
    "\n",
    "    Speech Recognition: In speech processing, MFCCs effectively capture the formant structure (resonant frequencies of the vocal tract) which is crucial for identifying phonemes (basic units of speech sound). This makes them extremely valuable for tasks like speech recognition and speaker identification.\n",
    "MFCC Values Over Time: The horizontal axis of the plot represents time. It shows how the MFCCs change throughout the duration of the audio file.\n",
    "\n",
    "MFCC Coefficients: The vertical axis represents the MFCC coefficients. The code calculates 13 coefficients (n_mfcc=13), so there are 13 rows in the plot, each row corresponding to one MFCC. MFCC 1 represents the average power in the audio signal. It can be related to the amplitude of the signal. MFCC 2 and 3 often relate to the broad shape of the spectral envelope. They can indicate aspects like the overall pitch and tone of the sound. Higher-Order MFCCs (4, 5, ...) represent finer spectral details. These coefficients can capture nuances in the sound, such as subtle changes in the vocal tract shape during speech, which are important for distinguishing different phonemes and sounds.\n",
    "\n",
    "Color-Encoded Magnitude: The color in the plot represents the magnitude of each MFCC coefficient at each point in time. The color scale is shown on the right side of the plot, with the color intensity (or brightness) indicating the magnitude. This is usually in decibels (dB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11efc42-fa70-49e3-811e-605b4962b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mfcc(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01174d-ce72-40af-aa98-6c14e3c73433",
   "metadata": {},
   "source": [
    "#### Pitch \n",
    "\n",
    "Pitch, often referred to as the fundamental frequency, is a key characteristic of the human voice that determines how high or low a sound is perceived. It is the rate at which vocal cords vibrate during phonation, and it plays a crucial role in communication, influencing aspects such as melody in speech, intonation, and the perceived emotion or intent behind words spoken. The fundamental frequency for adult male voices typically ranges from about 85 Hz to 155 Hz. The fundamental frequency for adult female voices generally ranges from about 165 Hz to 255 Hz. This range is often cited in various studies and textbooks on voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ecf7b-5cea-4f40-be71-6d4dc51bfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voice_pitch_with_rolling_average(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329f7c0-ab56-4a6b-ba6b-af9a799ef2c8",
   "metadata": {},
   "source": [
    "## Analysis Time\n",
    "\n",
    "The analyze_audio function performs a comprehensive analysis of an audio file, providing both transcription and various speech characteristics. It first openAI's whisper (a speech recognition model) to transcribe the audio to text. If analyze_by_sentence is True, it segments the audio by sentences and analyzes each segment separately, otherwise, it analyzes the whole audio file. The analysis includes determining the speaker's gender and emotion, and extracting speech metrics like the number of syllables, pauses, rate of speech, articulation rate, and vocal pitch characteristics (mean, standard deviation, median, minimum, maximum, and quantiles of fundamental frequency). The function returns these details either for each sentence or for the entire audio file, offering insights into the nuances of spoken language and vocal attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ea929-db68-48fc-96b9-7030f6ccf7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_by_sentence = True  # Set to False to analyze the whole audio\n",
    "file_path = r'C:\\Users\\...\\PietZwart\\downloaded_audio\\Prime_Minister_Trudeaus_message_on_Remembrance_Day.wav'\n",
    "analysis_results = analyze_audio(file_path, analyze_by_sentence)\n",
    "analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d9932-d150-44d3-afce-f6d9a6a36378",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plot(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f35708-8e7c-4a24-a851-6e51aac06c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_plot(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f114100-5889-44c4-bc5a-891741c7b655",
   "metadata": {},
   "source": [
    "#### Speaker Diarization\n",
    "\n",
    "**Why It's Interesting**:\n",
    "   - Speaker diarization is a key component in many audio analysis applications, such as transcribing meetings or interviews where multiple people are speaking. It allows for more accurate and speaker-specific transcriptions.\n",
    "   - Visualizing these segments can be very insightful, especially in understanding the dynamics of the conversation, like who dominates the conversation, how interactive it is, and the patterns of dialogue between participants.\n",
    "   - This process is crucial for audio data analysis in fields like psychology, linguistics, and communication studies, as well as for practical applications like automated meeting minutes and assistive technologies for the hearing impaired.\n",
    "     \n",
    "**Speaker Diarization**: \n",
    "   - The process of diarization involves identifying and segmenting an audio stream into homogenous segments according to the speaker identity. In simpler terms, it's about determining 'who spoke when' in an audio recording.\n",
    "   - The `Diarizer` object from a diarization library (possibly `pyAudioAnalysis` or similar) is used here. The model `'xvec'` is specified for embedding (feature extraction), and `'sc'` (possibly spectral clustering) for the clustering method.\n",
    "   - The `diarize` method is called on the `Diarizer` object with the path to the audio file (`file_path`) and the number of speakers (`num_speakers=2`). This method returns the segments of the audio file where each speaker is talking.\n",
    "\n",
    "**Reading and Combining the Waveplot**:\n",
    "   - `sf.read(file_path)` reads the audio file, returning the audio signal (`signal`) and its sample rate (`fs`).\n",
    "   - `combined_waveplot` is likely a function that plots the waveform of the audio signal and overlays the diarization segments onto this plot. This visualization shows when each speaker is talking during the audio file.\n",
    "\n",
    "In summary, this code is identifying different speakers in an audio file and then visualizing the timing and duration of each speaker's contributions, which is valuable for understanding spoken interactions in multi-speaker recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16691f93-fefd-4777-bd39-9deb5cc26332",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = Diarizer(embed_model='xvec', cluster_method='sc')\n",
    "segments = diarization.diarize(file_path, num_speakers=2)\n",
    "segments   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804443c-ad1b-492a-abba-95b0a1790996",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, fs = sf.read(file_path)\n",
    "combined_waveplot(signal, fs, segments)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfdb21-cb00-4930-b87c-f817ca96bf46",
   "metadata": {},
   "source": [
    "#### Speaker Similiarity\n",
    "\n",
    "The text below is from: https://github.com/OnTrack-UG-Squad/speaker-verification\n",
    "\n",
    "The Speaker Verification project aims to utilize machine learning technologies to evaluate audio files submitted from a user and obtain a confidence score of how likely it is that the voice in the recording is the user in question.\n",
    "\n",
    "#### Enroll stage\n",
    "\n",
    "The enroll workflow requires two parameters, one being a unique numeric id that must be 9 characters long and a path to a wav or flac file of the users voice. Below is the required syntax and format for the this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f929b83-380e-4a10-a576-e6cb18b69e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m speaker_verification enroll --id 123456789 --audio-path C:/Users/ahnji/OneDrive/Documents/Prototypes/PietZwart/downloaded_audio/Prime_Minister_Trudeaus_message_on_Remembrance_Day.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5617b-28e4-40ad-88a9-56d07b0f20fc",
   "metadata": {},
   "source": [
    "### Validate stage\n",
    "\n",
    "The validate workflow retrives a user enrollment based on the given id parameter given and then uses the --audio-path input to accept an audio file as speaker input to verify against the given user enrollment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202209f3-e6e0-4929-87ab-c0dc43d0dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m speaker_verification validate --id 123456789 --audio-path C:/Users/ahnji/OneDrive/Documents/Prototypes/PietZwart/downloaded_audio/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
